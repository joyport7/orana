{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "05fe5059-4c3f-4d4c-9883-f24e455fb3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseTitle:\n",
    "    def __init__(self,titles):\n",
    "        self.seplist = re.compile(\":\\s|-based|(?:\\s|^)(?:a|an|the|in|based on|on|to|of|with|without|by|from|for|via|toward|towards|beyond|using|and|or|as|is|am|are)(?:\\s|$)\",re.IGNORECASE)\n",
    "        self.replist = re.compile(\"^\\s|^(?:a|an|the|in)\\s|\\s(?:a|an|the|in)\\s|[^si](s)$|^\\s$|^in$\",re.IGNORECASE)\n",
    "        self.pulist = re.compile(\"(3d reconstruction|object detection|object recognition|object categorization|6d pose|face recognition|face detection|face verification|face identification|\\\n",
    "pedestrian detection|open dataset|knowledge distillation|style transfer|character recognition|character region detection|text spot|\\\n",
    "semantic segmentation|instance segmentation|image segmenation|object tracking|image retrieval|metric learning|pre training|fine tuning|fully convolutional network|Neural Radience Field|Light field|\\\n",
    "action recognition|re identification|image classification|deformable matching|deformable object|non　rigid|weak supervision|transformer|autoencoder|decoder|\\\n",
    "knowledge transfer|domain adapation|domain adaptation|reinforcement learning|transfer learning|active learning|multi modal|cross modal|\\\n",
    "simultaneous localization and mapping|federated learning|point cloud|Siamese|wild|disentangl|E2E|end to end|sparse|adversarial|weakly supervised|\\\n",
    "weak supervision|self supervis|one shot|few shot|zero shot|image generation|video generation|question answer|captioning|\\\n",
    "diffusion model|human computer interaction|attention|representation learning|contrastive learning|continual learning|incremental learning|\\\n",
    "unsupervised|semi supervised|noisy label|soft label|deep neural network|boosting|boost|expert|spatio temporal|\\\n",
    "multi task learning|multiple instance learning|multi instance learning|Kullback Leibler)s*\",re.IGNORECASE)\n",
    "        self.malist = re.compile(\"[^\\s-](SLAM|CNN|R CNN|RCNN|RNN|LSTM|CTC|SOM|GAN|ICA|PCA|U NET|UNET|Nerf|SVM|Bagging|MOE|\\\n",
    "Random forest|RF|KL divergence|DNN|GBM)s*[-\\s$]\")\n",
    "        self.plist = re.compile(\".+?\\s(?:interaction|estimation|prediction|alignment|detection|recognition|classification|segmentation|\\\n",
    "identification|retrieval|generation|prediction|inpainting|labeling|distillation|visualizaton|reconstruction|enhancement|\\\n",
    "editing|idenfication|verification|authentication|restoration|tracking|matching|categorization|denoising|navigation|Attack|Grounding|amplification|Captioning|\\\n",
    "answering|answer|Recovery|Parsing|localization|retargeting|relocalization|Synthesis|Correction|Searching|search|compression|Rendering|DeepFake|Reading|\\\n",
    "benchmark|benchmarking|animation|Translation)s*\",re.IGNORECASE)\n",
    "        self.mlist = re.compile(\"\\w+[-\\s]*(?:clustering|graph|training|transfer|Defocus|Regression|supression|synthetic|resolution|adaptation|network|\\\n",
    "fusion|reasoning|topology|space|modeling|model|learning|Feature|Generalization|Detector|\\\n",
    "classifier|recognizer|minimization|maxmization|Expression|label|Descriptor|extractor|sparcification|regularization|quantization|coarse to fine)s*|\\\n",
    "Independent Component\",re.IGNORECASE)\n",
    "        #llist = re.compile(\"\",re.IGNORECASE)\n",
    "        self.tlist = re.compile(\".*(?:3D|rgbd|video|frame|dataset|camera|infrared|depth|lider|3D|Text to Image|imagenet|wordnet)[-s]*\",re.IGNORECASE)\n",
    "        self.titles = titles\n",
    "        self.wordhist = {}\n",
    "\n",
    "    def wordfreq(self):\n",
    "        for title in self.titles:\n",
    "            #print(title)\n",
    "            sepwords = self.seplist.split(title)\n",
    "            bMatch = 0\n",
    "            for item in sepwords:\n",
    "                item = self.replist.sub(\"\", item)\n",
    "                item = re.sub( r'-' , \" \", item )\n",
    "                #####\n",
    "                words = self.pulist.findall(item)\n",
    "                if words:\n",
    "                    bMatch = 1\n",
    "                    for word in words:\n",
    "                        if word in self.wordhist:\n",
    "                            self.wordhist[word] += 1\n",
    "                        else:\n",
    "                            self.wordhist[word] = 1\n",
    "                if bMatch:\n",
    "                    continue\n",
    "                #####\n",
    "                words = self.malist.findall(item)\n",
    "                if words:\n",
    "                    bMatch = 1\n",
    "                    for word in words:\n",
    "                        if word in self.wordhist:\n",
    "                            self.wordhist[word] += 1\n",
    "                        else:\n",
    "                            self.wordhist[word] = 1\n",
    "                if bMatch:\n",
    "                    continue\n",
    "                #####\n",
    "                words = self.plist.findall(item)\n",
    "                if words:\n",
    "                    bMatch = 1\n",
    "                    for word in words:\n",
    "                        if word in self.wordhist:\n",
    "                            self.wordhist[word] += 1\n",
    "                        else:\n",
    "                            self.wordhist[word] = 1\n",
    "                if bMatch:\n",
    "                    continue\n",
    "                #####\n",
    "                words = self.mlist.findall(item)\n",
    "                if words:\n",
    "                    bMatch = 1\n",
    "                    for word in words:\n",
    "                        if word in self.wordhist:\n",
    "                            self.wordhist[word] += 1\n",
    "                        else:\n",
    "                            self.wordhist[word] = 1\n",
    "                if bMatch:\n",
    "                    continue\n",
    "                #####\n",
    "                words = self.tlist.findall(item)\n",
    "                if words:\n",
    "                    bMatch = 1\n",
    "                    for word in words:\n",
    "                        if word in self.wordhist:\n",
    "                            self.wordhist[word] += 1\n",
    "                        else:\n",
    "                            self.wordhist[word] = 1\n",
    "                if bMatch:\n",
    "                    continue\n",
    "\n",
    "                #if bMatch == 0 :\n",
    "                #    print(item)\n",
    "\n",
    "        stwordhist = sorted( self.wordhist.items(), key = lambda x:-x[1] )\n",
    "        return stwordhist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "8ae8b0b0-c032-4b2a-9e30-640510b951eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parse_ATlist:\n",
    "    def __init__(self, titles, authors):\n",
    "        self.num_papers = len(titles)\n",
    "        self.title_list = titles\n",
    "        self.author_list = authors\n",
    "        self.sjppat=\"^([AIUEO]|[BGKMNPR][aiueoAIUEO]|[BGKMNPR][yY][auoAUO]|D[aeoAEO]|S[aueoAUEO]|S[hH][aiouAIOU]|T[aeoAEO]|T[sS][uU]|Z[aueoAUEO]|Y[auoAUO]|\\\n",
    "H[aieoAIEO]|HY[auoAUO]|J[aiuoAIUO]|J[y][auoAUO]|C[hH][aiuoAIUO]|D[aeoAEO]|W[aA]|F[uU])\\\n",
    "([\\s'nmhtkNMHTK]|[aiueoAIUEO]|([bgkmnprBGKMNPR]|[kK][kK]|[mM][mM]|[nN][nN]|[pP][pP])[aiueoAIUEO]|([bgkmnprBGKMNPR]|[kK][kK]|[mM][mM]|[nN][nN]|\\\n",
    "[pP][pP])[yY][auoAUO]|[dD][aeoAEO]|([sS]|[sS][sS])[aueoAUEO]|([sS]|[sS][sS])[hH][aiouAIOU]\\\n",
    "|([tT]|[tT][tT])[aeoAEO]|([tT]|[tT][tT])[sS][uU]|[zZ][aueoAUEO]|[yY][auoAUO]|[hH][aieoAIEO]|[hH][yY][auoAUO]|[jJ][aiuoAIUO]|[jJ][y][auoAUO]|([cC]|[cC][cC])[hH][aiuoAIUO]|[dD][aeoAEO]|[wW][aA]|[fF][uU])*?$\"\n",
    "        self.snjpwpat=\"[^n]'|-|[QXVqxv]|[áčçøåÅÇØ]|[^aiueontkh]\\s|[^aiueontkh]$|nn\\s|nn$|\\\n",
    "[aA]er|ahn|[bBjJ]iao|[bB]odo|([cC]h|[bdjBDJ])ae|[cC]hoe|[eE]ui|[eE]un|eau|[fF][ieo]|[hH]ao|[hH]oai|[hH]yun|[hH]yuk|[yY]oun|guk|[jJ]ia|[hH]ua|Niu|[jJ]ie[^in]|[hkgHKG]yun|nea|\\\n",
    "[sS]hui|rian|[sSgG]uo|[sS]eo[nk]|[sS]han|[sS]eung|[sS]huai|uan|\\\n",
    "([fF][yY]u)an$|\\\n",
    "[rsy]ani|rini|[rR]oha|gudo|[zZ]ere|[kK]ede|[wW]ah|[aA]hn|\\\n",
    "[bdfghjklmpqrvwxyzBCDFGHJKLMPQRTVWXYZ][bcdfgjklmpqrtvwxz]|c[^h]|[tT][^aeos]|s[^aueoh]|[sS]h[^aiuo]|[tT]s[^u]|[in]h$|[jJ]as\"\n",
    "        self.snjppat=\"\\\n",
    "(([JN]|Sh|Ch)i|Bin|[GKd]e|Bo|Hai|[TH]ao|Bei|Ku[in]|\\\n",
    "Akanksha|Amanda|Ameya|Amit|Anita|Anki|Ankita|Anton|Antonio|Anuja|Aran|Arya|Arun|Aida|Aren|Arezou|Aria|Ata|\\\n",
    "Bao|Babak|Bee|Beibei|Benjamin|Bodo|Burak|Bukun|\\\n",
    "Chaehun|Chajin|Chanjin|Chau|Chiara|Da|Dakai|Dami[ae]n|Dann*a|Dario|Debora|Depanshu|Dohwan|Dominik|Dongoh|Doreen|Doron|\\\n",
    "Erik|Ehsan|Eugene|Gyumin|Han|Hanbin|Hani|Hanna|Hannah|Hasan|Heiko|Hogun|\\\n",
    "Inkyu|Itai|Jamie|Jan|Jane|Janne|Jason|Jimin|Jirak|Jose|John|Joao|Joni|Junni|Juho|Juhee|Junho|Junhyuk|Kaan|Kannan|Kanika|Karan|Kate|Kenan|Kenneth|Kiana|Kien|Koushik|Leda|Lorenzo|\\\n",
    "Maatei|Madonna|Man|Manikanta|Marion|Matan|Mateja|Matt|Mazen|Meena|Meike|Mihai|Minho*|Minji|Min[jJ]oon|Min[jJ]un*|Minsu|Minkyo|Mubarak|Mohi|Moon|Nageen|Namo|Nikita|Nirat|\\\n",
    "Oren|Ori|Oladayo|Pere|Pu|Ramana|Rami|Renshuai|Reza|Ruben|Rohit|Roi|Ruwan|Ruhi|Ryan|Sachin|Sage|Saikat|Sandaru|Sami|Samitha|Samira|Sarah|Sayan|Sean|Sen|Shasha|Shayak|Shayan|Shai|Shichao|Shujon|Soomin|Suman|Suren|Sunita|Taha|Taman|Tanaya|Tapani|Tarasha|\\\n",
    "Udana|Wah|Yani|Yanrui|Yasaman|Yohan|Yuanbiao|Yue|Yun|Yuru|Yunhao|Zarena|Zarana|Zerui|Zeyu)\\s|\\\n",
    "((Ch|N)a|(Ch|Sh|[YSHFOLGMB])[uo]|[YSHFOLGMKZ]e|[EH]o|An|([TYHGW]|[BDL]|Shu)ai|[BGHYMR]ao|([YRG]u|Ch|[LYHTWGNM])an|[BNJ]i|Jia|[JM]iao|[GS]ui|[YS]un|[KGZ]uo|[LM]ei|Heo|[YH]ou|Choi|Yoo|Yoon|\\\n",
    "Achan|Arora|Ahuja|Aksak|Akshara|Amani|Amini|Amit|Amin|Antebi|\\\n",
    "Badea|Basu|Bilen|Bahadori|Bandara|Ben|Benjamin|Bera|Bian|Bin|Bok|Boneh|Bouman|Bui|Choo|Desai|Dami[ae]n|Damaine|Do|Eak|Egiazarian|Emonet|Esen|Gat|Gia|Gu|Guha|Guhan|\\\n",
    "Ha|Hakimi|Hansen|Heidari|Himayat|Huh|Hui|Huo|Ho|Ie|Imani|Jagadeesan|Jain|Jaipuria|Jamnik|Jansen|Jayasuriya|Juge|John|Johnson|Joo|Joshi|Ju|Junge|\\\n",
    "Kahn|Kahnsari|Kee|Khurana|Ki|Kiani|Kijak|Kishore|Kunze|\\\n",
    "Laina|Ma|Madan|Maman|Mania|Mangoubi|Mannone|Masera|Mason|Masoomi|Mazaheri|Min|Mingyu|Miolane|Memari|Men|Mohan|Mok|Monteiro|Moore|\\\n",
    "Narayan|Narayanan|Neeman|Nayak|Nabi|Naik|Natarajan|Nauta|Nizan|Neumann|Ngo|\\\n",
    "Oh|Ohana|Oza|Pai|Pajarinen|Pan|Pei|Podee|Popa|Poranne|Pore|Poria|Potapenko|Pu|Rai|Raja|Rajan|Rajawat|Ramamohanarao|Ramyaa|Rane|Rau|Rakshit|Razazadeh|Razani|Rohani|Ren|Riba|Ridao|Roh|Ruta|Ryu|\\\n",
    "Sabo|Saria|Saenko|Saha|Sain|Saini|Samano|Samara.+?|Sani|Saran|Sarawagi|Shabanian|Shah|Shayani|Shi|Shin|Shou|Sohn|Souza|Suh|Sundararaman|Suri|Suin|Tandon|Tonekabori|\\\n",
    "Wanchoo|Watson|Yaesoubi|Yogatama|Yoran|Yue|Yu[ae]n|Zada|Zabih|Zadeh|Zaharia|Zou)$|\\\n",
    "Jin\\sJin|Ran\\sTao|Ran\\sBen|Yuhe\\sJin|Jin\\sTao|Yu\\sTao|Jan\\sKotera|Ken\\sMai|Meina\\sKan|Jinko\\sKanno|Rui\\sTao|^.+?\\s.+?\\s.+?$|\\\n",
    "Joon|Hee\" \n",
    "        self.dlm=\",\\s\"\n",
    "\n",
    "    def isJP(self,name):\n",
    "        # extract JP name\n",
    "        rmat = re.search(self.sjppat,name)\n",
    "        rstop = re.search(self.snjpwpat,name)\n",
    "        rnstop = re.search(self.snjppat,name)\n",
    "        if rmat and not rstop and not rnstop: # Japanese-like list and Non-Japanese name filtering\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def selectJP(self):\n",
    "        jpauthor = []\n",
    "        jptitle = []\n",
    "        jppaper = 0\n",
    "        \n",
    "        for ii in range(0,self.num_papers):\n",
    "            ttl = self.title_list[ii]\n",
    "            jpp = False\n",
    "            for au in self.author_list[ii]:\n",
    "                # print(au)\n",
    "                if self.isJP(au):\n",
    "                    # print(au)\n",
    "                    jpauthor.append(au)\n",
    "                    jptitle.append(ttl)\n",
    "                    jpp = True\n",
    "            if jpp:\n",
    "                jppaper += 1\n",
    "                \n",
    "        return jppaper, jptitle, jpauthor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "c22d9530-a93d-4b42-ac9b-825a03b4f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_binary\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "class parse_url:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        print('\\n==================')\n",
    "        print(url)\n",
    "        yr = re.search('[0-9][0-9][0-9][0-9]',url)\n",
    "        print(yr[0])\n",
    "        if yr[0] == '2018':\n",
    "            self.oralheld = True\n",
    "            self.slheld = False\n",
    "            self.posterheld = True\n",
    "            self.oralpt = 'accepted-oral-papers'\n",
    "            self.posterpt = 'accepted-poster-papers'\n",
    "            self.slpt = ''\n",
    "        elif yr[0] == '2019':\n",
    "            self.oralheld = True\n",
    "            self.slheld = False\n",
    "            self.posterheld = True\n",
    "            self.oralpt = 'oral-presentations'\n",
    "            self.posterpt = 'poster-presentations'\n",
    "            self.slpt = ''\n",
    "        elif yr[0] == '2020':\n",
    "            self.oralheld = True\n",
    "            self.slheld = True\n",
    "            self.posterheld = True\n",
    "            self.oralpt = 'oral-presentations'\n",
    "            self.posterpt = 'poster-presentations'\n",
    "            self.slpt = 'spotlight-presentations'\n",
    "        elif yr[0] == '2021':\n",
    "            self.oralheld = True\n",
    "            self.slheld = True\n",
    "            self.posterheld = True\n",
    "            self.oralpt = 'oral-presentations'\n",
    "            self.posterpt = 'poster-presentations'\n",
    "            self.slpt = 'spotlight-presentations'\n",
    "        elif yr[0] == '2022':\n",
    "            self.oralheld = True\n",
    "            self.slheld = True\n",
    "            self.posterheld = True\n",
    "            self.oralpt = 'oral-submissions'\n",
    "            self.posterpt = 'poster-submissions'\n",
    "            self.slpt = 'spotlight-submissions'\n",
    "        \n",
    "    def selenium(self):\n",
    "        time_s = time.time()\n",
    "\n",
    "        fname = re.sub(\"[\\:\\/\\.]\",\"_\",self.url) + \".txt\"\n",
    "        is_file = os.path.isfile(fname)\n",
    "        if is_file:\n",
    "            with open(fname, encoding='utf-8') as f:\n",
    "                bs = BeautifulSoup(f.read(), 'html.parser')\n",
    "        else:\n",
    "            bs_oral = []\n",
    "            bs_sl = []\n",
    "            bs_poster = []\n",
    "            opt = Options()\n",
    "            pt = 8\n",
    "\n",
    "            oralpt = self.oralpt\n",
    "            slpt = self.slpt\n",
    "            posterpt = self.posterpt\n",
    "            oralheld = self.oralheld\n",
    "            slheld = self.slheld\n",
    "            posterheld = self.posterheld\n",
    "\n",
    "            url_oral = self.url + '#' + oralpt\n",
    "            url_sl = self.url + '#' + slpt\n",
    "            url_poster = self.url + '#' + posterpt\n",
    "\n",
    "            #opt.add_argument(\"--headless\") # headless mode\n",
    "\n",
    "            #oral\n",
    "            if oralheld:\n",
    "                print('oral...')\n",
    "                try:\n",
    "                    driver = webdriver.Chrome(options = opt)\n",
    "                    driver.get(url_oral)\n",
    "                    time.sleep(pt)\n",
    "                    bs_oral.append(BeautifulSoup(driver.page_source.encode('utf-8'), 'html.parser'))\n",
    "                    #print(bs_oral[0].prettify())\n",
    "                    #with open(fname, mode='w') as f:\n",
    "                    #    f.write(str(bs_oral[0].prettify()))\n",
    "                    a_item = driver.find_element(By.CSS_SELECTOR, (\"div[id=\"+oralpt+\"]\"))\n",
    "                    elems = a_item.find_elements(By.TAG_NAME, (\"nav\"))\n",
    "                    if len(elems) != 0:\n",
    "                        a_items = elems[0]\n",
    "                        a_items = a_item.find_elements(By.CLASS_NAME, (\"right-arrow\"))\n",
    "                        max_page = int(a_items[1].get_attribute(\"data-page-number\"))\n",
    "                        print(max_page,' pages')\n",
    "                        #print(1)\n",
    "                        print(end='#')\n",
    "                        for ii in range(1,max_page):\n",
    "                            #print(ii+1)\n",
    "                            print(end='#')\n",
    "                            a_item = driver.find_element(By.CSS_SELECTOR, (\"div[id=\"+oralpt+\"]\"))\n",
    "                            a_item = a_item.find_element(By.TAG_NAME, (\"nav\"))\n",
    "                            a_item = a_item.find_element(By.LINK_TEXT, str(ii+1))\n",
    "                            a_item.click()\n",
    "                            time.sleep(pt)\n",
    "                            bs_oral.append(BeautifulSoup(driver.page_source.encode('utf-8'), 'html.parser'))\n",
    "                except driver.exceptions.RequestException as e:\n",
    "                    print(\"Error: \",e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            else:\n",
    "                bs_oral = None\n",
    "\n",
    "\n",
    "            #spotlight\n",
    "            if slheld:\n",
    "                print('spotlight...')\n",
    "                try:\n",
    "                    driver = webdriver.Chrome(options = opt)\n",
    "                    driver.get(url_sl)\n",
    "                    time.sleep(pt)\n",
    "\n",
    "                    bs_sl.append(BeautifulSoup(driver.page_source.encode('utf-8'), 'html.parser'))\n",
    "\n",
    "                    a_item = driver.find_element(By.CSS_SELECTOR, (\"div[id=\"+slpt+\"]\"))\n",
    "                    elems = a_item.find_elements(By.TAG_NAME, (\"nav\"))\n",
    "                    if len(elems) != 0:\n",
    "                        a_items = elems[0]\n",
    "                        a_items = a_item.find_elements(By.CLASS_NAME, (\"right-arrow\"))\n",
    "                        max_page = int(a_items[1].get_attribute(\"data-page-number\"))\n",
    "                        print(max_page,' pages')\n",
    "                        #print(1)\n",
    "                        print(end='#')\n",
    "                        for ii in range(1,max_page):\n",
    "                            #print(ii+1)\n",
    "                            print(end='#')\n",
    "                            a_item = driver.find_element(By.CSS_SELECTOR, (\"div[id=\"+slpt+\"]\"))\n",
    "                            a_item = a_item.find_element(By.TAG_NAME, (\"nav\"))\n",
    "                            a_item = a_item.find_element(By.LINK_TEXT, (str(ii+1)))\n",
    "                            a_item.click()\n",
    "                            time.sleep(pt)\n",
    "                            bs_sl.append(BeautifulSoup(driver.page_source.encode('utf-8'), 'html.parser'))\n",
    "                except driver.exceptions.RequestException as e:\n",
    "                    print(\"Error: \",e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            else:\n",
    "                bs_sl = None\n",
    "\n",
    "            #poster\n",
    "            if posterheld:\n",
    "                print('poster...')\n",
    "                try:\n",
    "                    driver = webdriver.Chrome(options = opt)\n",
    "                    driver.get(url_poster)\n",
    "                    time.sleep(pt)\n",
    "                    bs_poster.append(BeautifulSoup(driver.page_source.encode('utf-8'), 'html.parser'))\n",
    "                    #print(driver.page_source)\n",
    "                    a_item = driver.find_element(By.CSS_SELECTOR, (\"div[id=\"+posterpt+\"]\"))\n",
    "                    elems = a_item.find_elements(By.TAG_NAME, (\"nav\"))\n",
    "                    if len(elems) != 0:\n",
    "                        a_items = elems[0]\n",
    "                        a_items = a_item.find_elements(By.CLASS_NAME, (\"right-arrow\"))\n",
    "                        max_page = int(a_items[1].get_attribute(\"data-page-number\"))\n",
    "                        print(max_page,' pages')\n",
    "                        #print(1)\n",
    "                        print(end='#')\n",
    "                        for ii in range(1,max_page):\n",
    "                            #print(ii+1)\n",
    "                            print(end='#')\n",
    "                            a_item = driver.find_element(By.CSS_SELECTOR, (\"div[id=\"+posterpt+\"]\"))\n",
    "                            a_item = a_item.find_element(By.TAG_NAME, (\"nav\"))\n",
    "                            a_item = a_item.find_element(By.LINK_TEXT, str(ii+1))\n",
    "                            a_item.click()\n",
    "                            time.sleep(pt)\n",
    "                            bs_poster.append(BeautifulSoup(driver.page_source.encode('utf-8'), 'html.parser'))\n",
    "                except driver.exceptions.RequestException as e:\n",
    "                    print(\"Error: \",e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            else:\n",
    "                bs_poster = None\n",
    "\n",
    "        time_e = time.time()\n",
    "        etime = time_e - time_s\n",
    "        print(\"elapsed time: {0:.2f}\".format(etime))\n",
    "        return bs_oral, bs_sl, bs_poster\n",
    "            \n",
    "    def beautifulsoup(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Error: \",e)\n",
    "\n",
    "        bs = BeautifulSoup(response.text, 'html.parser')\n",
    "        return bs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "736c288c-c118-4d5e-9382-27b0b134913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICLR(url):\n",
    "    pu = parse_url(url)\n",
    "    bs_orals, bs_sls, bs_posters = pu.selenium()\n",
    "    \n",
    "    #a aria-controls=\"oral-submissions\"\n",
    "    title_list = []\n",
    "    author_list = []\n",
    "\n",
    "    #-----------\n",
    "    numoral = 0\n",
    "    if bs_orals:\n",
    "        for ii in range(0,len(bs_orals)):\n",
    "            oralblock = bs_orals[ii].find(\"div\",{\"id\":pu.oralpt})\n",
    "            tts = oralblock.find_all(\"h4\")\n",
    "            aus = oralblock.find_all(\"div\",{\"class\":\"note-authors\"})\n",
    "\n",
    "            numoral += len(tts)\n",
    "            for jj in range(0,len(tts)):\n",
    "                title_list.append(tts[jj].get_text().strip())\n",
    "                author_list.append(aus[jj].get_text().strip().split(', '))\n",
    "        print(numoral)\n",
    "            \n",
    "    #-----------\n",
    "    numsl = 0\n",
    "    if bs_sls:\n",
    "        for ii in range(0,len(bs_sls)):\n",
    "            slblock = bs_sls[ii].find(\"div\",{\"id\":pu.slpt})\n",
    "            tts = slblock.find_all(\"h4\")\n",
    "            aus = slblock.find_all(\"div\",{\"class\":\"note-authors\"})\n",
    "\n",
    "            numsl += len(tts)\n",
    "            for jj in range(0,len(tts)):\n",
    "                title_list.append(tts[jj].get_text().strip())\n",
    "                author_list.append(aus[jj].get_text().strip().split(', '))\n",
    "        print(numsl)\n",
    "    \n",
    "    #-----------\n",
    "    numposter = 0\n",
    "    if bs_posters:\n",
    "        for ii in range(0,len(bs_posters)):\n",
    "            posterblock = bs_posters[ii].find(\"div\",{\"id\":pu.posterpt})\n",
    "            tts = posterblock.find_all(\"h4\")\n",
    "            aus = posterblock.find_all(\"div\",{\"class\":\"note-authors\"})\n",
    "\n",
    "            numposter += len(tts)\n",
    "            for jj in range(0,len(tts)):\n",
    "                title_list.append(tts[jj].get_text().strip())\n",
    "                author_list.append(aus[jj].get_text().strip().split(', '))\n",
    "        print(numposter)\n",
    "    \n",
    "    #print(bs.prettify())\n",
    "    \n",
    "    return title_list, author_list, numoral, numsl, numposter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "37ec68c7-c649-4878-8b80-a7d4242073f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      "https://openreview.net/group?id=ICLR.cc/2019/Conference\n",
      "2019\n",
      "oral...\n",
      "poster...\n",
      "elapsed time: 26.71\n",
      "24\n",
      "478\n",
      "---------------------\n",
      "2019\tnum papers\t502\tnum papers (JPN)\t11\n",
      "---------------------\n",
      "Akihiro Matsukawa\tDo Deep Generative Models Know What They Don't Know?\n",
      "Shoichiro Yamaguchi\tDISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS\n",
      "Masanori Koyama\tDISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS\n",
      "Heiga Zen\tHierarchical Generative Modeling for Controllable Speech Synthesis\n",
      "Taiji Suzuki\tAdaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality\n",
      "Sho Yaida\tFluctuation-dissipation relations for stochastic gradient descent\n",
      "Masahiro Kato\tLearning from Positive and Unlabeled Data with a Selection Bias\n",
      "Takeshi Teshima\tLearning from Positive and Unlabeled Data with a Selection Bias\n",
      "Junya Honda\tLearning from Positive and Unlabeled Data with a Selection Bias\n",
      "Masashi Sugiyama\tOn the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data\n",
      "Takayuki Osa\tHierarchical Reinforcement Learning via Advantage-Weighted Information Maximization\n",
      "Masashi Sugiyama\tHierarchical Reinforcement Learning via Advantage-Weighted Information Maximization\n",
      "Heiga Zen\tSample Efficient Adaptive Text-to-Speech\n",
      "Fumihiro Sasaki\tSample Efficient Imitation Learning for Continuous Control\n",
      "Tetsuya Yohira\tSample Efficient Imitation Learning for Continuous Control\n",
      "Makoto Yamada\tPost Selection Inference with Incomplete Maximum Mean Discrepancy Estimator\n",
      "Ichiro Takeuchi\tPost Selection Inference with Incomplete Maximum Mean Discrepancy Estimator\n",
      "Kenji Fukumizu\tPost Selection Inference with Incomplete Maximum Mean Discrepancy Estimator\n",
      "Akihiro Matsukawa\t1\n",
      "Shoichiro Yamaguchi\t1\n",
      "Masanori Koyama\t1\n",
      "Heiga Zen\t2\n",
      "Taiji Suzuki\t1\n",
      "Sho Yaida\t1\n",
      "Masahiro Kato\t1\n",
      "Takeshi Teshima\t1\n",
      "Junya Honda\t1\n",
      "Masashi Sugiyama\t2\n",
      "Takayuki Osa\t1\n",
      "Fumihiro Sasaki\t1\n",
      "Tetsuya Yohira\t1\n",
      "Makoto Yamada\t1\n",
      "Ichiro Takeuchi\t1\n",
      "Kenji Fukumizu\t1\n",
      "\n",
      "==================\n",
      "https://openreview.net/group?id=ICLR.cc/2020/Conference\n",
      "2020\n",
      "oral...\n",
      "spotlight...\n",
      "poster...\n",
      "elapsed time: 38.81\n",
      "48\n",
      "108\n",
      "531\n",
      "---------------------\n",
      "2020\tnum papers\t687\tnum papers (JPN)\t16\n",
      "---------------------\n",
      "Eiichiro Sumita\tData-dependent Gaussian Prior Objective for Language Generation\n",
      "Taiji Suzuki\tGeneralization of Two-layer Neural Networks: An Asymptotic Viewpoint\n",
      "Taiji Suzuki\tCompression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network\n",
      "Hiroshi Abe\tCompression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network\n",
      "Tomoaki Nishimura\tCompression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network\n",
      "Eiichiro Sumita\tNeural Machine Translation with Universal Visual Representation\n",
      "Kenta Oono\tGraph Neural Networks Exponentially Lose Expressive Power for Node Classification\n",
      "Taiji Suzuki\tGraph Neural Networks Exponentially Lose Expressive Power for Node Classification\n",
      "Ryota Tomioka\tConservative Uncertainty Estimation By Fitting  Prior Networks\n",
      "Kazuki Yoshiyama\tMixed Precision DNNs: All you need is a good parametrization\n",
      "Akira Nakamura\tMixed Precision DNNs: All you need is a good parametrization\n",
      "Sanchari Sen\tEMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness Against Adversarial Attacks\n",
      "Pouya Samangouei\tDiscrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth\n",
      "Keisuke Sakaguchi\tAbductive Commonsense Reasoning\n",
      "Akari Asai\tLearning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\n",
      "Kazuma Hashimoto\tLearning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\n",
      "Atsuhiro Noguchi\tRGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis\n",
      "Tatsuya Harada\tRGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis\n",
      "Kentaro Minami\tSmoothness and Stability in GANs\n",
      "Kenji Fukumizu\tSmoothness and Stability in GANs\n",
      "Kikuo Fujimura\tCM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning\n",
      "Kenji Doya\tVariational Recurrent Models for Solving Partially Observable Control Tasks\n",
      "Jun Tani\tVariational Recurrent Models for Solving Partially Observable Control Tasks\n",
      "Yuu Jinnai\tExploration in Reinforcement Learning with Deep Covering Options\n",
      "Eiichiro Sumita\t2\n",
      "Taiji Suzuki\t3\n",
      "Hiroshi Abe\t1\n",
      "Tomoaki Nishimura\t1\n",
      "Kenta Oono\t1\n",
      "Ryota Tomioka\t1\n",
      "Kazuki Yoshiyama\t1\n",
      "Akira Nakamura\t1\n",
      "Sanchari Sen\t1\n",
      "Pouya Samangouei\t1\n",
      "Keisuke Sakaguchi\t1\n",
      "Akari Asai\t1\n",
      "Kazuma Hashimoto\t1\n",
      "Atsuhiro Noguchi\t1\n",
      "Tatsuya Harada\t1\n",
      "Kentaro Minami\t1\n",
      "Kenji Fukumizu\t1\n",
      "Kikuo Fujimura\t1\n",
      "Kenji Doya\t1\n",
      "Jun Tani\t1\n",
      "Yuu Jinnai\t1\n",
      "\n",
      "==================\n",
      "https://openreview.net/group?id=ICLR.cc/2021/Conference\n",
      "2021\n",
      "oral...\n",
      "spotlight...\n",
      "poster...\n",
      "elapsed time: 37.93\n",
      "53\n",
      "114\n",
      "0\n",
      "---------------------\n",
      "2021\tnum papers\t167\tnum papers (JPN)\t10\n",
      "---------------------\n",
      "Atsushi Nitanda\tOptimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime\n",
      "Taiji Suzuki\tOptimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime\n",
      "Masashi Sugiyama\tGeometry-aware Instance-reweighted Adversarial Training\n",
      "Taiji Suzuki\tBenefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods\n",
      "Shunta Akiyama\tBenefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods\n",
      "Naoyuki Terashita\tInfluence Estimation for Generative Adversarial Networks\n",
      "Hiroki Ohashi\tInfluence Estimation for Generative Adversarial Networks\n",
      "Yuichi Nonaka\tInfluence Estimation for Generative Adversarial Networks\n",
      "Takashi Kanemaru\tInfluence Estimation for Generative Adversarial Networks\n",
      "Fumihiro Sasaki\tBehavioral Cloning from Noisy Demonstrations\n",
      "Ryota Yamashina\tBehavioral Cloning from Noisy Demonstrations\n",
      "Kenji Kawaguchi\tHow Does Mixup Help With Robustness and Generalization?\n",
      "Marisa Kirisame\tDynamic Tensor Rematerialization\n",
      "Kenji Kawaguchi\tOn the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers\n",
      "Yuki Asano\tSupport-set bottlenecks for video-text representation learning\n",
      "Taiki Miyagawa\tSequential Density Ratio Estimation for Simultaneous Optimization of Speed and Accuracy\n",
      "Kazuyuki Sakurai\tSequential Density Ratio Estimation for Simultaneous Optimization of Speed and Accuracy\n",
      "Hitoshi Imaoka\tSequential Density Ratio Estimation for Simultaneous Optimization of Speed and Accuracy\n",
      "Atsushi Nitanda\t1\n",
      "Taiji Suzuki\t2\n",
      "Masashi Sugiyama\t1\n",
      "Shunta Akiyama\t1\n",
      "Naoyuki Terashita\t1\n",
      "Hiroki Ohashi\t1\n",
      "Yuichi Nonaka\t1\n",
      "Takashi Kanemaru\t1\n",
      "Fumihiro Sasaki\t1\n",
      "Ryota Yamashina\t1\n",
      "Kenji Kawaguchi\t2\n",
      "Marisa Kirisame\t1\n",
      "Yuki Asano\t1\n",
      "Taiki Miyagawa\t1\n",
      "Kazuyuki Sakurai\t1\n",
      "Hitoshi Imaoka\t1\n",
      "\n",
      "==================\n",
      "https://openreview.net/group?id=ICLR.cc/2022/Conference\n",
      "2022\n",
      "oral...\n",
      "2  pages\n",
      "##spotlight...\n",
      "4  pages\n",
      "####poster...\n",
      "18  pages\n",
      "##################elapsed time: 222.14\n",
      "55\n",
      "175\n",
      "864\n",
      "---------------------\n",
      "2022\tnum papers\t1094\tnum papers (JPN)\t36\n",
      "---------------------\n",
      "Shiori Sagawa\tExtending the WILDS Benchmark for Unsupervised Adaptation\n",
      "Michihiro Yasunaga\tExtending the WILDS Benchmark for Unsupervised Adaptation\n",
      "Tatsunori Hashimoto\tExtending the WILDS Benchmark for Unsupervised Adaptation\n",
      "Kohei Miyaguchi\tVariational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion\n",
      "Takayuki Katsuki\tVariational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion\n",
      "Akira Koseki\tVariational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion\n",
      "Toshiya Iwamori\tVariational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion\n",
      "Tatsunori Hashimoto\tLarge Language Models Can Be Strong Differentially Private Learners\n",
      "Tatsunori Hashimoto\tLanguage modeling via stochastic processes\n",
      "Takashi Mori\tStrength of Minibatch Noise in SGD\n",
      "Masahito Ueda\tStrength of Minibatch Noise in SGD\n",
      "Michihiro Yasunaga\tGreaseLM: Graph REASoning Enhanced Language Models\n",
      "Soji Adeshina\tDoes your graph need a confidence boost?  Convergent boosted smoothing on graphs with tabular node features\n",
      "Sho Okumoto\tLearnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness\n",
      "Taiji Suzuki\tLearnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness\n",
      "Masatoshi Uehara\tRepresentation Learning for Online and Offline RL in Low-rank MDPs\n",
      "Masahiro Kato\tLearning Causal Models from Conditional Moment Restrictions by Importance Weighting\n",
      "Masaaki Imaizumi\tLearning Causal Models from Conditional Moment Restrictions by Importance Weighting\n",
      "Shota Yasui\tLearning Causal Models from Conditional Moment Restrictions by Importance Weighting\n",
      "Haruo Kakehi\tLearning Causal Models from Conditional Moment Restrictions by Importance Weighting\n",
      "Masashi Sugiyama\tMeta Discovery: Learning to Discover Novel Classes given Very Limited Data\n",
      "Hiroki Furuta\tGeneralized Decision Transformer for Offline Hindsight Information Matching\n",
      "Takayoshi Asakura\tSuperclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings\n",
      "Tomoya Soma\tSuperclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings\n",
      "Sho Kato\tSuperclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings\n",
      "Masahito Ueda\tSGD Can Converge to Local Maxima\n",
      "Ryutaro Tanno\tLearning to Downsample for Segmentation of Ultra-High Resolution Images\n",
      "Tsubasa Takahashi\tPEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning\n",
      "Michihiko Ueno\tPEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning\n",
      "Ryuichi Kanoh\tA Neural Tangent Kernel Perspective of Infinite Tree Ensembles\n",
      "Mahito Sugiyama\tA Neural Tangent Kernel Perspective of Infinite Tree Ensembles\n",
      "Taiji Suzuki\tUnderstanding the Variance Collapse of SVGD in High Dimensions\n",
      "Masahito Ueda\tConvergent and Efficient Deep Q Learning Algorithm\n",
      "Masashi Sugiyama\tSample Selection with Uncertainty of Losses for Learning with Noisy Labels\n",
      "Kazusato Oko\tParticle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization\n",
      "Taiji Suzuki\tParticle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization\n",
      "Atsushi Nitanda\tParticle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization\n",
      "Masataka Sawayama\tLanguage-biased image classification: evaluation based on semantic representations\n",
      "Tatsunori Hashimoto\tIs Importance Weighting Incompatible with Interpolating Classifiers?\n",
      "Masashi Sugiyama\tFederated Learning from Only Unlabeled Data with Class-conditional-sharing Clients\n",
      "Kazuki Irie\tThe Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization\n",
      "Tatsunori Hashimoto\tDistributionally Robust Models with Parametric Likelihood Ratios\n",
      "Masashi Sugiyama\tExploiting Class Activation Value for Partial-Label Learning\n",
      "Seiya Tokui\tDisentanglement Analysis with Partial Information Decomposition\n",
      "Ryo Karakida\tLearning curves for continual learning in neural networks: Self-knowledge transfer and forgetting\n",
      "Shotaro Akaho\tLearning curves for continual learning in neural networks: Self-knowledge transfer and forgetting\n",
      "Masatoshi Uehara\tPessimistic Model-based Offline Reinforcement Learning under Partial Coverage\n",
      "Jun Yamada\tTask-Induced Representation Learning\n",
      "Takuya Hiraoka\tDropout Q-Functions for Doubly Efficient Reinforcement Learning\n",
      "Takahisa Imagawa\tDropout Q-Functions for Doubly Efficient Reinforcement Learning\n",
      "Taisei Hashimoto\tDropout Q-Functions for Doubly Efficient Reinforcement Learning\n",
      "Takashi Onishi\tDropout Q-Functions for Doubly Efficient Reinforcement Learning\n",
      "Yoshimasa Tsuruoka\tDropout Q-Functions for Doubly Efficient Reinforcement Learning\n",
      "Masashi Sugiyama\tRethinking Class-Prior Estimation for Positive-Unlabeled Learning\n",
      "Kenji Fukumizu\t$\\beta$-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap\n",
      "Norio Kosaka\tKnow Your Action Set: Learning Action Relations for Reinforcement Learning\n",
      "Tadashi Kozuno\tVariational oracle guiding for reinforcement learning\n",
      "Kenji Doya\tVariational oracle guiding for reinforcement learning\n",
      "Shiori Sagawa\t1\n",
      "Michihiro Yasunaga\t2\n",
      "Tatsunori Hashimoto\t5\n",
      "Kohei Miyaguchi\t1\n",
      "Takayuki Katsuki\t1\n",
      "Akira Koseki\t1\n",
      "Toshiya Iwamori\t1\n",
      "Takashi Mori\t1\n",
      "Masahito Ueda\t3\n",
      "Soji Adeshina\t1\n",
      "Sho Okumoto\t1\n",
      "Taiji Suzuki\t3\n",
      "Masatoshi Uehara\t2\n",
      "Masahiro Kato\t1\n",
      "Masaaki Imaizumi\t1\n",
      "Shota Yasui\t1\n",
      "Haruo Kakehi\t1\n",
      "Masashi Sugiyama\t5\n",
      "Hiroki Furuta\t1\n",
      "Takayoshi Asakura\t1\n",
      "Tomoya Soma\t1\n",
      "Sho Kato\t1\n",
      "Ryutaro Tanno\t1\n",
      "Tsubasa Takahashi\t1\n",
      "Michihiko Ueno\t1\n",
      "Ryuichi Kanoh\t1\n",
      "Mahito Sugiyama\t1\n",
      "Kazusato Oko\t1\n",
      "Atsushi Nitanda\t1\n",
      "Masataka Sawayama\t1\n",
      "Kazuki Irie\t1\n",
      "Seiya Tokui\t1\n",
      "Ryo Karakida\t1\n",
      "Shotaro Akaho\t1\n",
      "Jun Yamada\t1\n",
      "Takuya Hiraoka\t1\n",
      "Takahisa Imagawa\t1\n",
      "Taisei Hashimoto\t1\n",
      "Takashi Onishi\t1\n",
      "Yoshimasa Tsuruoka\t1\n",
      "Kenji Fukumizu\t1\n",
      "Norio Kosaka\t1\n",
      "Tadashi Kozuno\t1\n",
      "Kenji Doya\t1\n",
      "---------------------\n",
      "2019\n",
      "---------------------\n",
      "Heiga Zen\t2\n",
      "Masashi Sugiyama\t2\n",
      "Akihiro Matsukawa\t1\n",
      "Shoichiro Yamaguchi\t1\n",
      "Masanori Koyama\t1\n",
      "Taiji Suzuki\t1\n",
      "Sho Yaida\t1\n",
      "Masahiro Kato\t1\n",
      "Takeshi Teshima\t1\n",
      "Junya Honda\t1\n",
      "Takayuki Osa\t1\n",
      "Fumihiro Sasaki\t1\n",
      "Tetsuya Yohira\t1\n",
      "Makoto Yamada\t1\n",
      "Ichiro Takeuchi\t1\n",
      "Kenji Fukumizu\t1\n",
      "---------------------\n",
      "2020\n",
      "---------------------\n",
      "Taiji Suzuki\t3\n",
      "Eiichiro Sumita\t2\n",
      "Hiroshi Abe\t1\n",
      "Tomoaki Nishimura\t1\n",
      "Kenta Oono\t1\n",
      "Ryota Tomioka\t1\n",
      "Kazuki Yoshiyama\t1\n",
      "Akira Nakamura\t1\n",
      "Sanchari Sen\t1\n",
      "Pouya Samangouei\t1\n",
      "Keisuke Sakaguchi\t1\n",
      "Akari Asai\t1\n",
      "Kazuma Hashimoto\t1\n",
      "Atsuhiro Noguchi\t1\n",
      "Tatsuya Harada\t1\n",
      "Kentaro Minami\t1\n",
      "Kenji Fukumizu\t1\n",
      "Kikuo Fujimura\t1\n",
      "Kenji Doya\t1\n",
      "Jun Tani\t1\n",
      "Yuu Jinnai\t1\n",
      "---------------------\n",
      "2021\n",
      "---------------------\n",
      "Taiji Suzuki\t2\n",
      "Kenji Kawaguchi\t2\n",
      "Atsushi Nitanda\t1\n",
      "Masashi Sugiyama\t1\n",
      "Shunta Akiyama\t1\n",
      "Naoyuki Terashita\t1\n",
      "Hiroki Ohashi\t1\n",
      "Yuichi Nonaka\t1\n",
      "Takashi Kanemaru\t1\n",
      "Fumihiro Sasaki\t1\n",
      "Ryota Yamashina\t1\n",
      "Marisa Kirisame\t1\n",
      "Yuki Asano\t1\n",
      "Taiki Miyagawa\t1\n",
      "Kazuyuki Sakurai\t1\n",
      "Hitoshi Imaoka\t1\n",
      "---------------------\n",
      "Taiji Suzuki\t6\n",
      "Masashi Sugiyama\t3\n",
      "Heiga Zen\t2\n",
      "Fumihiro Sasaki\t2\n",
      "Kenji Fukumizu\t2\n",
      "Eiichiro Sumita\t2\n",
      "Kenji Kawaguchi\t2\n"
     ]
    }
   ],
   "source": [
    "urlpre = 'https://openreview.net/group?id=ICLR.cc/'\n",
    "urlpost = '/Conference'\n",
    "\n",
    "since = 2019\n",
    "until = 2022\n",
    "\n",
    "ttlist = {}\n",
    "aulist = {}\n",
    "numor = {}\n",
    "numsl = {}\n",
    "numpo = {}\n",
    "numppr = {}\n",
    "jpppr = {}\n",
    "jptitles = {}\n",
    "jpauthors = {}\n",
    "hist = {}\n",
    "\n",
    "for ii in range(since,until+1,1):\n",
    "    yr = str(ii)\n",
    "    url = urlpre+yr+urlpost\n",
    "    \n",
    "    ttlist[yr], aulist[yr], numor[yr], numsl[yr], numpo[yr] = ICLR(url)\n",
    "    pAT = parse_ATlist(ttlist[yr], aulist[yr])\n",
    "    jpppr[yr],jptitles[yr],jpauthors[yr] = pAT.selectJP()\n",
    "    numppr[yr] = len(ttlist[yr])\n",
    "\n",
    "    #print('==============')\n",
    "    #print('RESULT')\n",
    "    #print('numppr\\t{0:}\\t numjpppr\\t{1:}'.format(numppr,jpppr[yr]))\n",
    "    #print('==============')\n",
    "    #for jj in range(0,len(jptitles[yr])):\n",
    "    #    print('{0:}\\t{1:}'.format(jptitles[yr][jj],jpauthors[yr][jj]))\n",
    "\n",
    "    for author in jpauthors[yr]:\n",
    "        if yr in hist.keys():\n",
    "            if author in hist[yr].keys():\n",
    "                hist[yr][author] += 1\n",
    "            else:\n",
    "                hist[yr][author] = 1\n",
    "        else:\n",
    "            hist[yr] = {author:1}\n",
    "\n",
    "    print('---------------------')\n",
    "    print('{0}\\tnum papers\\t{1}\\tnum papers (JPN)\\t{2}'.format(yr,numppr[yr],jpppr[yr]))\n",
    "    print('---------------------')\n",
    "    for jj in range(0,len(jpauthors[yr])):\n",
    "        print('{0:}\\t{1:}'.format(jpauthors[yr][jj],jptitles[yr][jj]))\n",
    "    for author in hist[yr].keys():\n",
    "        print('{0:}\\t{1:}'.format(author,hist[yr][author]))\n",
    "\n",
    "minpub = 2\n",
    "oahist = {}\n",
    "for ii in range(since,until,1):\n",
    "    sthist = sorted( hist[str(ii)].items(), key = lambda x:x[1], reverse = True )\n",
    "    print('---------------------')\n",
    "    print(str(ii))\n",
    "    print('---------------------')\n",
    "    for jj in range(0,len(sthist),1):\n",
    "        print(\"{0:}\\t{1:}\".format(sthist[jj][0], sthist[jj][1]))\n",
    "    for jj in range(0,len(sthist),1):\n",
    "        #print(\"{0:}\\t{1:}\".format(sthist[jj][0], sthist[jj][1]))\n",
    "        if sthist[jj][0] in oahist:\n",
    "            oahist[sthist[jj][0]] += sthist[jj][1]\n",
    "        else:\n",
    "            oahist[sthist[jj][0]] = sthist[jj][1]\n",
    "\n",
    "print('---------------------')\n",
    "sthist = sorted( oahist.items(), key = lambda x:x[1], reverse = True )\n",
    "for ii in range(0,len(sthist),1):\n",
    "    if sthist[ii][1] >= minpub:\n",
    "        print(\"{0:}\\t{1:}\".format(sthist[ii][0], sthist[ii][1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1eb7d1-96a7-423c-ae64-79aff530c3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
